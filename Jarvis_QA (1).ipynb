{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisities"
      ],
      "metadata": {
        "id": "djUueNlS3Ybz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWGJyfzj2kp6",
        "outputId": "6fb35caa-d9a2-42e5-e40d-cf5b9b12388d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/0.PhD/Jarvis-QA\"\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/0')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SoIuIyn2s0A",
        "outputId": "37dacf42-7325-44d8-f5c0-4861d0719cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test.jsonl  train.jsonl  validation.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! sudo apt -y -qq install tesseract-ocr libtesseract-dev\n",
        "! sudo apt-get -y -qq install poppler-utils\n",
        "! pip install -U unstructured==0.7.5 pdf2image==1.16.3 pytesseract==0.3.10 pdfminer.six\n",
        "! pip install -U tensorflow_hub==0.13.0 tensorflow_text==2.12.1\n",
        "! pip install jq\n",
        "! pip install -U google-cloud-aiplatform==1.35.0 langchain==0.0.312\n",
        "! pip install chromadb==0.4.15"
      ],
      "metadata": {
        "id": "-v42j5b13Fa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Automatically restart kernel after installs so that your environment can access the new packages\n",
        " import IPython\n",
        "\n",
        " app = IPython.Application.instance()\n",
        " app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8Sxa5MZ3a64",
        "outputId": "672cdb62-65fd-48e8-a16f-49f288ca20d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Colab Authentication\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "Y3ak1lqa3rJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vertex AI\n",
        "from google.cloud import aiplatform\n",
        "import vertexai\n",
        "\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
        "import langchain\n",
        "\n",
        "print(f\"LangChain version: {langchain.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDvjRwaZ3tIJ",
        "outputId": "5434f012-5376-4c95-b000-419ba3787749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertex AI SDK version: 1.35.0\n",
            "LangChain version: 0.0.312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GCP Configurations"
      ],
      "metadata": {
        "id": "isS3Em68B3AA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Google Cloud Project\n",
        "PROJECT_ID = \"alkali-gworks\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# Initialize Vertex AI SDK\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)"
      ],
      "metadata": {
        "id": "hR1VsDcF4WD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Library Imports\n",
        "import urllib\n",
        "import warnings\n",
        "from pathlib import Path as p\n",
        "from pprint import pprint\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "from langchain.llms import VertexAI\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n"
      ],
      "metadata": {
        "id": "vrXh-pCm4los"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialise Vertex AI Models"
      ],
      "metadata": {
        "id": "fpbq9ARuB95Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Bison Model from Vertex AI based on PaLM v2\n",
        "vertex_llm_text = VertexAI(model_name=\"text-bison@001\")\n",
        "\n",
        "# Text Embedding Gecko Model from Vertex AI based on PaLM v2\n",
        "vertex_embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@001\")\n"
      ],
      "metadata": {
        "id": "-DlS_8B44cgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preprocessing/Loading"
      ],
      "metadata": {
        "id": "PWv7oF8K4wrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset is loaded to Google Drive path and imported\n",
        "dataset_path = '/content/drive/MyDrive/0.PhD/Jarvis-QA/train.jsonl'\n",
        "from pathlib import Path\n",
        "\n",
        "pprint(Path(dataset_path).read_text())\n",
        "\n"
      ],
      "metadata": {
        "id": "j8COgI9l4ep_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 JSON Loader (Langchain)"
      ],
      "metadata": {
        "id": "V__LIvdU5Hl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Langchain based JSON loader to load dataset in JSON Lines format\n",
        "dataset_path = '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl'\n",
        "from langchain.document_loaders import JSONLoader\n",
        "loader = JSONLoader(\n",
        "    file_path=dataset_path,\n",
        "    jq_schema='.answers[].sents[].text',\n",
        "    text_content=False,\n",
        "    json_lines=True)\n",
        "\n",
        "data = loader.load()\n",
        "data[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raeexa3R49kT",
        "outputId": "07ee50e0-a522-48a5-8dcd-96d89991df2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"Yes it's fine\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 1}),\n",
              " Document(page_content=\"It's fairly normal to ask someone to proof\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 2}),\n",
              " Document(page_content=\"-read your resume or to help with layout, so there's nothing wrong there.\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 3}),\n",
              " Document(page_content=\"There's so many resumes sent out with elementary mistakes, so another set of eyes really helps in getting things right.\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 4}),\n",
              " Document(page_content=\"If the recruitment is happening via an agency, the hiring company won't see your resume, they'll just see a copy/paste of the relevant parts (recruiters won't want the client seeing your contact details).\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 5})]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Retrieval Augmented Generation\n",
        "\n",
        "Information Retrieval combined with Generative LLMs"
      ],
      "metadata": {
        "id": "7z04brcV97FZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Stuffing\n",
        "\n",
        "Adding the entire document into the Prompt as Context\n",
        "\n"
      ],
      "metadata": {
        "id": "5uftBVpv-e2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Restricting Data to 10 Rows for catering to API limit (1024 Tokens)\n",
        "context = \"\\n\".join(str(p.page_content) for p in data[:10])\n",
        "print(\"The total words in the context: \", len(context))\n",
        "question = \"Is it acceptable to format a cv for someone else?\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfUL03_r9BO9",
        "outputId": "2e9de93e-e2a1-449c-dbc8-4f2dbd508c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total words in the context:  729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompting\n",
        "prompt_template = \"\"\"Answer the question as precise as possible using the provided context. If the answer is\n",
        "                    not contained in the context, say \"answer not available in context\" \\n\\n\n",
        "                    Context: \\n {context}?\\n\n",
        "                    Question: \\n {question} \\n\n",
        "                    Answer:\n",
        "                  \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "HMfXzkyn-ler"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unrestricted Data\n",
        "context = \"\\n\".join(str(p.page_content) for p in data)\n",
        "print(\"The total words in the context: \", len(context))\n",
        "\n",
        "# QA Chain\n",
        "stuff_chain = load_qa_chain(vertex_llm_text, chain_type=\"stuff\", prompt=prompt)\n",
        "stuff_answer = stuff_chain(\n",
        "    {\"input_documents\": data, \"question\": question}, return_only_outputs=True\n",
        ")\n",
        "pprint(stuff_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "61osAgfOBPNv",
        "outputId": "4eee67f0-6b58-4436-fac8-f7c99ab17771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total words in the context:  1905079\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgument",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         )\n\u001b[0;32m-> 1161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=not-instantiable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"The request cannot be processed. The most likely reason is that the provided input exceeded the model's input token limit.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:173.194.217.95:443 {created_time:\"2023-12-03T21:35:04.891699153+00:00\", grpc_status:3, grpc_message:\"The request cannot be processed. The most likely reason is that the provided input exceeded the model\\'s input token limit.\"}\"\n>",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-d2571638fdb7>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# QA Chain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstuff_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_qa_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertex_llm_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"stuff\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m stuff_answer = stuff_chain(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"input_documents\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_only_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             outputs = (\n\u001b[0;32m--> 300\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Other keys are assumed to be needed for LLM prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mother_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_key\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         output, extra_return_dict = self.combine_docs(\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mother_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/stuff.py\u001b[0m in \u001b[0;36mcombine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# Call predict on the LLM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     async def acombine_docs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mcompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"funny\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \"\"\"\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             outputs = (\n\u001b[0;32m--> 300\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     ) -> Dict[str, str]:\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;34m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    497\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 )\n\u001b[1;32m    646\u001b[0m             ]\n\u001b[0;32m--> 647\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    648\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             output = (\n\u001b[0;32m--> 522\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    523\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/vertexai.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[0mgenerations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                 res = completion_with_retry(\n\u001b[0m\u001b[1;32m    286\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/vertexai.py\u001b[0m in \u001b[0;36mcompletion_with_retry\u001b[0;34m(llm, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mis_explicit_retry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTryAgain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_explicit_retry\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/vertexai.py\u001b[0m in \u001b[0;36m_completion_with_retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mretry_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vertexai/language_models/_language_models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, prompt, max_output_tokens, temperature, top_k, top_p, stop_sequences, candidate_count)\u001b[0m\n\u001b[1;32m    772\u001b[0m         )\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m         prediction_response = self._endpoint.predict(\n\u001b[0m\u001b[1;32m    775\u001b[0m             \u001b[0minstances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprediction_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, instances, parameters, timeout, use_raw_predict)\u001b[0m\n\u001b[1;32m   1594\u001b[0m             )\n\u001b[1;32m   1595\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1596\u001b[0;31m             prediction_response = self._prediction_client.predict(\n\u001b[0m\u001b[1;32m   1597\u001b[0m                 \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m                 \u001b[0minstances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    605\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgument\u001b[0m: 400 The request cannot be processed. The most likely reason is that the provided input exceeded the model's input token limit."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Restricting Data to 10 Rows for catering to API limit (1024 Tokens)\n",
        "stuff_chain = load_qa_chain(vertex_llm_text, chain_type=\"stuff\", prompt=prompt)\n",
        "stuff_answer = stuff_chain(\n",
        "    {\"input_documents\": data[:10], \"question\": question}, return_only_outputs=True\n",
        ")\n",
        "pprint(stuff_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwBtsekfAaiG",
        "outputId": "c9b2c7ec-a17d-44f8-d4ee-01010480889c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'output_text': 'Yes, of course.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Map Reduce\n",
        "Applies an LLM chain to each document individually (the Map step), treating the chain output as a new document.\n",
        "\n",
        "Passes all the new documents to a separate combine documents chain to get a single output (the Reduce step)"
      ],
      "metadata": {
        "id": "AdwytdRlC1-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Restricting Data to 40 Rows for catering to API limit (1024 Tokens)\n",
        "context = \"\\n\".join(str(p.page_content) for p in data[:40])\n",
        "print(\"The total words in the context: \", len(context))\n",
        "\n",
        "#question = \"What potential issues are highlighted by the user 'Old_Lamplighter' when it comes to proofreading and formatting a CV for someone else?\"\n",
        "question = \"What potential issues are highlighted by the user 'Old_Lamplighter'?\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gezT1c4C0mp",
        "outputId": "158128a3-0b86-4126-ef53-08522fae21c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total words in the context:  3250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompting\n",
        "question_prompt_template = \"\"\"Answer the question as precise as possible using the provided context. If the answer is not contained in the context, say \"answer not available in context\"\n",
        "                    Context: \\n {context} \\n\n",
        "                    Question: \\n {question} \\n\n",
        "                    Answer:\n",
        "                    \"\"\"\n",
        "question_prompt = PromptTemplate(\n",
        "    template=question_prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "pprint(question_prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDZ8zkppC_PE",
        "outputId": "d7fb8c46-161d-4f3f-d7c7-89d5e822af6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PromptTemplate(input_variables=['context', 'question'], template='Answer the question as precise as possible using the provided context. If the answer is not contained in the context, say \"answer not available in context\" \\n                    Context: \\n {context} \\n\\n                    Question: \\n {question} \\n\\n                    Answer:\\n                    ')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined Prompt\n",
        "combine_prompt_template = \"\"\"Given the extracted content and the question, create a final answer.\n",
        "If the answer is not contained in the context, say \"answer not available in context. \\n\\n\n",
        "Summaries: \\n {summaries}?\\n\n",
        "Question: \\n {question} \\n\n",
        "Answer:\n",
        "\"\"\"\n",
        "combine_prompt = PromptTemplate(\n",
        "    template=combine_prompt_template, input_variables=[\"summaries\", \"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "tl6VdYQJDDJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QA Chain\n",
        "\n",
        "map_reduce_chain = load_qa_chain(\n",
        "    vertex_llm_text,\n",
        "    chain_type=\"map_reduce\",\n",
        "    return_intermediate_steps=True,\n",
        "    question_prompt=question_prompt,\n",
        "    combine_prompt=combine_prompt,\n",
        ")"
      ],
      "metadata": {
        "id": "eU5fj4wpEJjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unrestricted Data with Mapreduce\n",
        "map_reduce_outputs = map_reduce_chain({\"input_documents\": data, \"question\": question})\n",
        "map_reduce_outputs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "Q2JdjCnhEMqa",
        "outputId": "12021be2-4796-4b4c-9cc6-41f48eb6713c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.llms.vertexai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.vertexai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas..\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-9ffb62c93291>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmap_reduce_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_reduce_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"input_documents\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmap_reduce_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             outputs = (\n\u001b[0;32m--> 300\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Other keys are assumed to be needed for LLM prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mother_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_key\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         output, extra_return_dict = self.combine_docs(\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mother_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/map_reduce.py\u001b[0m in \u001b[0;36mcombine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mThis\u001b[0m \u001b[0mreducing\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdone\u001b[0m \u001b[0mrecursively\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mneeded\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmany\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         map_results = self.llm_chain.apply(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;31m# FYI - this is parallelized and so it is fast.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_variable_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    186\u001b[0m         )\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;34m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    497\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 )\n\u001b[1;32m    646\u001b[0m             ]\n\u001b[0;32m--> 647\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    648\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             output = (\n\u001b[0;32m--> 522\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    523\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/vertexai.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[0mgenerations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                 res = completion_with_retry(\n\u001b[0m\u001b[1;32m    286\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/vertexai.py\u001b[0m in \u001b[0;36mcompletion_with_retry\u001b[0;34m(llm, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoSleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_for_next_attempt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdo\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/nap.py\u001b[0m in \u001b[0;36msleep\u001b[0;34m(seconds)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmocked\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0munit\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \"\"\"\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Restricting Data to 40 Rows for catering to API limit\n",
        "map_reduce_outputs = map_reduce_chain({\"input_documents\": data[:40], \"question\": question})\n",
        "map_reduce_outputs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtQVyTiCEQmr",
        "outputId": "69f0290d-0eac-4808-f380-7ee59e330e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_documents': [Document(page_content=\"Yes it's fine\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 1}),\n",
              "  Document(page_content=\"It's fairly normal to ask someone to proof\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 2}),\n",
              "  Document(page_content=\"-read your resume or to help with layout, so there's nothing wrong there.\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 3}),\n",
              "  Document(page_content=\"There's so many resumes sent out with elementary mistakes, so another set of eyes really helps in getting things right.\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 4}),\n",
              "  Document(page_content=\"If the recruitment is happening via an agency, the hiring company won't see your resume, they'll just see a copy/paste of the relevant parts (recruiters won't want the client seeing your contact details).\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 5}),\n",
              "  Document(page_content=\"In general, it's far more important to get the facts right than for it to look pretty.\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 6}),\n",
              "  Document(page_content='Is it acceptable to format a cv for someone else?', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 7}),\n",
              "  Document(page_content='Yes, of course.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 8}),\n",
              "  Document(page_content='I help friends write and format their resumes all the time.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 9}),\n",
              "  Document(page_content='Nobody knows or cares who wrote or formatted your CV/resume.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 10}),\n",
              "  Document(page_content='They only care that it accurately reflects you, your background, and your career.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 11}),\n",
              "  Document(page_content=\"I can't imagine that an interviewer would ask about a nice resume.\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 12}),\n",
              "  Document(page_content='But if they did, saying \"Well, I had a friend who is really good at it', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 13}),\n",
              "  Document(page_content='give me a hand.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 14}),\n",
              "  Document(page_content='\" is a perfectly acceptable answer.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 15}),\n",
              "  Document(page_content='Yes , its fine to help someone with their resume/cv.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 16}),\n",
              "  Document(page_content='I have been complimented on my resume for its format and such in the past, but never has an interviewer asked how I came up with it.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 17}),\n",
              "  Document(page_content='It is very low risk in my opinion to whomever your helping.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 18}),\n",
              "  Document(page_content='In fact, there are several companies who make a living by creating resumes for professionals at all levels.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 19}),\n",
              "  Document(page_content='Their resume is the first step, the golden ticket if you will, to earn the interview.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 20}),\n",
              "  Document(page_content='I would do whatever I felt necessary to make mine as solid as possible.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 21}),\n",
              "  Document(page_content='Formatting and content both matter .', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 22}),\n",
              "  Document(page_content='Think of it as their first impression with the potential employer.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 23}),\n",
              "  Document(page_content='Do what you can to make is as good as it can be.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 24}),\n",
              "  Document(page_content='Just to go against the grain...', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 25}),\n",
              "  Document(page_content='It can possibly be a problem, IF.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 26}),\n",
              "  Document(page_content=\"You use words or phraseology that the person doesn't normally use You embellish\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 27}),\n",
              "  Document(page_content='The person cannot recite it from memory.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 28}),\n",
              "  Document(page_content=\"If he doesn't know what's on his own resume, that's a BIG red flag\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 29}),\n",
              "  Document(page_content='It doesn\\'t \"fit\" the job applied for.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 30}),\n",
              "  Document(page_content='You want your resume to be fine tuned to hit the requirements of the job for which you are applying.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 31}),\n",
              "  Document(page_content='To avoid this, make sure you go over the CV as you are making the changes, and the person understands them and can go over it with you line by line and memorize it.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 32}),\n",
              "  Document(page_content='Edited to add: To expand on my point #1.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 33}),\n",
              "  Document(page_content=\"If I am interviewing someone and the language at the interview doesn't match what I've seen on the resume, it's going to make me think that something is wrong, that I'm not interviewing the same person I saw on the resume.\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 34}),\n",
              "  Document(page_content=\"It is absolutely acceptable to proofread someone's CV/resume.\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 35}),\n",
              "  Document(page_content='However, if you are really asking about \"crafting\" a resume, that\\'s a different matter.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 36}),\n",
              "  Document(page_content='By \"crafted\" resume I mean a type of resume, which, while not making any obviously false claims, presents the facts in a misleading manner.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 37}),\n",
              "  Document(page_content='For example, a resume may suggest that the candidate is a rocket scientist, while actually he worked at Goddard Space Flight Center as a janitor.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 38}),\n",
              "  Document(page_content=\"So, what can go bad is that the reviewer (or should we call him co-author?) might have his best intentions and embellish the candidate's profile, but the hiring manager would not be impressed.\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 39}),\n",
              "  Document(page_content='The remedy is simple - stick to the facts, and ask yourself, if you were the hiring manager yourself, would you like this candidate?', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 40})],\n",
              " 'question': \"What potential issues are highlighted by the user 'Old_Lamplighter'?\",\n",
              " 'intermediate_steps': ['answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'elementary mistakes',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'Nobody knows or cares who wrote or formatted your CV/resume.',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'It doesn\\'t \"fit\" the job applied for.',\n",
              "  'answer not available in context',\n",
              "  'The person may not understand the changes and may not be able to memorize them.',\n",
              "  'answer not available in context',\n",
              "  \"The language at the interview doesn't match what I've seen on the resume.\",\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'answer not available in context',\n",
              "  'a resume may suggest that the candidate is a rocket scientist, while actually he worked at Goddard Space Flight Center as a janitor.',\n",
              "  \"the reviewer might have his best intentions and embellish the candidate's profile, but the hiring manager would not be impressed\",\n",
              "  'answer not available in context'],\n",
              " 'output_text': 'a resume may suggest that the candidate is a rocket scientist, while actually he worked at Goddard Space Flight Center as a janitor.'}"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "map_reduce_outputs['output_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Vfo4Z_o7nUzn",
        "outputId": "74802d27-3c83-48ad-b49a-f0cd46a2be45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a resume may suggest that the candidate is a rocket scientist, while actually he worked at Goddard Space Flight Center as a janitor.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#########"
      ],
      "metadata": {
        "id": "BjT4Zu6rEnIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Refine\n",
        "\n",
        "Looping over the input documents and iteratively updating its answer.\n",
        "\n",
        "For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer.\n",
        "\n"
      ],
      "metadata": {
        "id": "5TN_DCVSFjJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Restricting Data to 50 Rows for catering to API limit\n",
        "context = \"\\n\".join(str(p.page_content) for p in data[:50])\n",
        "print(\"The total words in the context: \", len(context))\n",
        "\n",
        "question = \"What potential issues are highlighted by the user 'Old_Lamplighter'?\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1mufEAiGM76",
        "outputId": "3da34ebc-e427-48c4-bab8-b6d5dc0d7a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total words in the context:  4186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "refine_prompt_template = \"\"\"\n",
        "    The original question is: \\n {question} \\n\n",
        "    The provided answer is: \\n {existing_answer}\\n\n",
        "    Refine the existing answer if needed with the following context: \\n {context_str} \\n\n",
        "    Given the extracted content and the question, create a final answer.\n",
        "    If the answer is not contained in the context, say \"answer not available in context. \\n\\n\n",
        "\"\"\"\n",
        "refine_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"existing_answer\", \"context_str\"],\n",
        "    template=refine_prompt_template,\n",
        ")\n",
        "refine_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S0GCbEhFl_H",
        "outputId": "02457fa1-bfc6-41a3-b2f4-90e3674c3399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['question', 'existing_answer', 'context_str'], template='\\n    The original question is: \\n {question} \\n\\n    The provided answer is: \\n {existing_answer}\\n\\n    Refine the existing answer if needed with the following context: \\n {context_str} \\n\\n    Given the extracted content and the question, create a final answer.\\n    If the answer is not contained in the context, say \"answer not available in context. \\n\\n\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_question_prompt_template = \"\"\"\n",
        "    Answer the question as precise as possible using the provided context only.\n",
        "    If the answer is not contained in the context, say \"answer not available in context. \\n\\n\n",
        "    Context: \\n {context_str} \\n\n",
        "    Question: \\n {question} \\n\n",
        "    Answer:\n",
        "\"\"\"\n",
        "\n",
        "initial_question_prompt = PromptTemplate(\n",
        "    input_variables=[\"context_str\", \"question\"],\n",
        "    template=initial_question_prompt_template,\n",
        ")\n",
        "initial_question_prompt_template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "kjn0OO3oFpUQ",
        "outputId": "dfd823ca-132d-47a1-811d-fc83aa3e4d0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    Answer the question as precise as possible using the provided context only. \\n    If the answer is not contained in the context, say \"answer not available in context. \\n\\n\\n    Context: \\n {context_str} \\n\\n    Question: \\n {question} \\n\\n    Answer:\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain\n",
        "refine_chain = load_qa_chain(\n",
        "    vertex_llm_text,\n",
        "    chain_type=\"refine\",\n",
        "    return_intermediate_steps=True,\n",
        "    question_prompt=initial_question_prompt,\n",
        "    refine_prompt=refine_prompt,\n",
        ")\n"
      ],
      "metadata": {
        "id": "AFDFslPhFvGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "refine_outputs = refine_chain({\"input_documents\": data[:50], \"question\": question})\n",
        "refine_outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsReuiNbF8Zq",
        "outputId": "e6558ca4-7c26-4006-ef52-51c4be0e9c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_documents': [Document(page_content=\"Yes it's fine\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 1}),\n",
              "  Document(page_content=\"It's fairly normal to ask someone to proof\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 2}),\n",
              "  Document(page_content=\"-read your resume or to help with layout, so there's nothing wrong there.\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 3}),\n",
              "  Document(page_content=\"There's so many resumes sent out with elementary mistakes, so another set of eyes really helps in getting things right.\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 4}),\n",
              "  Document(page_content=\"If the recruitment is happening via an agency, the hiring company won't see your resume, they'll just see a copy/paste of the relevant parts (recruiters won't want the client seeing your contact details).\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 5}),\n",
              "  Document(page_content=\"In general, it's far more important to get the facts right than for it to look pretty.\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 6}),\n",
              "  Document(page_content='Is it acceptable to format a cv for someone else?', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 7}),\n",
              "  Document(page_content='Yes, of course.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 8}),\n",
              "  Document(page_content='I help friends write and format their resumes all the time.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 9}),\n",
              "  Document(page_content='Nobody knows or cares who wrote or formatted your CV/resume.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 10}),\n",
              "  Document(page_content='They only care that it accurately reflects you, your background, and your career.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 11}),\n",
              "  Document(page_content=\"I can't imagine that an interviewer would ask about a nice resume.\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 12}),\n",
              "  Document(page_content='But if they did, saying \"Well, I had a friend who is really good at it', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 13}),\n",
              "  Document(page_content='give me a hand.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 14}),\n",
              "  Document(page_content='\" is a perfectly acceptable answer.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 15}),\n",
              "  Document(page_content='Yes , its fine to help someone with their resume/cv.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 16}),\n",
              "  Document(page_content='I have been complimented on my resume for its format and such in the past, but never has an interviewer asked how I came up with it.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 17}),\n",
              "  Document(page_content='It is very low risk in my opinion to whomever your helping.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 18}),\n",
              "  Document(page_content='In fact, there are several companies who make a living by creating resumes for professionals at all levels.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 19}),\n",
              "  Document(page_content='Their resume is the first step, the golden ticket if you will, to earn the interview.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 20}),\n",
              "  Document(page_content='I would do whatever I felt necessary to make mine as solid as possible.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 21}),\n",
              "  Document(page_content='Formatting and content both matter .', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 22}),\n",
              "  Document(page_content='Think of it as their first impression with the potential employer.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 23}),\n",
              "  Document(page_content='Do what you can to make is as good as it can be.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 24}),\n",
              "  Document(page_content='Just to go against the grain...', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 25}),\n",
              "  Document(page_content='It can possibly be a problem, IF.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 26}),\n",
              "  Document(page_content=\"You use words or phraseology that the person doesn't normally use You embellish\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 27}),\n",
              "  Document(page_content='The person cannot recite it from memory.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 28}),\n",
              "  Document(page_content=\"If he doesn't know what's on his own resume, that's a BIG red flag\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 29}),\n",
              "  Document(page_content='It doesn\\'t \"fit\" the job applied for.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 30}),\n",
              "  Document(page_content='You want your resume to be fine tuned to hit the requirements of the job for which you are applying.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 31}),\n",
              "  Document(page_content='To avoid this, make sure you go over the CV as you are making the changes, and the person understands them and can go over it with you line by line and memorize it.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 32}),\n",
              "  Document(page_content='Edited to add: To expand on my point #1.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 33}),\n",
              "  Document(page_content=\"If I am interviewing someone and the language at the interview doesn't match what I've seen on the resume, it's going to make me think that something is wrong, that I'm not interviewing the same person I saw on the resume.\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 34}),\n",
              "  Document(page_content=\"It is absolutely acceptable to proofread someone's CV/resume.\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 35}),\n",
              "  Document(page_content='However, if you are really asking about \"crafting\" a resume, that\\'s a different matter.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 36}),\n",
              "  Document(page_content='By \"crafted\" resume I mean a type of resume, which, while not making any obviously false claims, presents the facts in a misleading manner.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 37}),\n",
              "  Document(page_content='For example, a resume may suggest that the candidate is a rocket scientist, while actually he worked at Goddard Space Flight Center as a janitor.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 38}),\n",
              "  Document(page_content=\"So, what can go bad is that the reviewer (or should we call him co-author?) might have his best intentions and embellish the candidate's profile, but the hiring manager would not be impressed.\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 39}),\n",
              "  Document(page_content='The remedy is simple - stick to the facts, and ask yourself, if you were the hiring manager yourself, would you like this candidate?', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 40}),\n",
              "  Document(page_content='Limited answer - this is not fine if...', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 41}),\n",
              "  Document(page_content='If the C.V. or job description describes the person as being good at design, formatting documents, or experienced with LaTeX, then the person should format it themselves.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 42}),\n",
              "  Document(page_content='In this (very limited) case, the implicit assumption the company will come away with is that you are presenting your C.V. as an example of your work.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 43}),\n",
              "  Document(page_content='Having someone else format it for you is likely to lead them to a false conclusion.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 44}),\n",
              "  Document(page_content='If not in this situation, see any of the other answers.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 45}),\n",
              "  Document(page_content='My concern is that if I send my application to an employer through email, it is likely that he/she would not be able to read it thoroughly, given the number of application emails he/ she receives.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 46}),\n",
              "  Document(page_content='But would sending my application through a courier hurt my chances of landing the job?', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 47}),\n",
              "  Document(page_content='Bad idea.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 48}),\n",
              "  Document(page_content='You would look foolish.', metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 49}),\n",
              "  Document(page_content=\"And you probably wouldn't increase the chances that your resume would be read any differently than any other resume.\", metadata={'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'seq_num': 50})],\n",
              " 'question': \"What potential issues are highlighted by the user 'Old_Lamplighter'?\",\n",
              " 'intermediate_steps': ['answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'Nobody knows or cares who wrote or formatted your CV/resume.',\n",
              "  '',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'It doesn\\'t \"fit\" the job applied for.',\n",
              "  'It doesn\\'t \"fit\" the job applied for.',\n",
              "  'It doesn\\'t \"fit\" the job applied for.',\n",
              "  'It doesn\\'t \"fit\" the job applied for.',\n",
              "  'It doesn\\'t \"fit\" the job applied for.',\n",
              "  'It doesn\\'t \"fit\" the job applied for.',\n",
              "  'It doesn\\'t \"fit\" the job applied for.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.',\n",
              "  'answer not available in context.'],\n",
              " 'output_text': 'answer not available in context.'}"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "refine_outputs['output_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Vth2jQSLF9yA",
        "outputId": "fc0e92fd-ceec-42af-d63b-2454b15d2cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'answer not available in context.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######"
      ],
      "metadata": {
        "id": "s0vjzcgynsV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. Vector Stores\n",
        "\n",
        "Embed it and store the resulting embedding vectors in Vector DB\n",
        "\n",
        "At query time, Embed the unstructured query and Retrieve the embedding vectors that are 'most similar' to the embedded query."
      ],
      "metadata": {
        "id": "ohVL69G_GdKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200, chunk_overlap=50, add_start_index=True\n",
        ")\n",
        "documents = text_splitter.split_documents(data[:1000])\n",
        "vector_index = Chroma.from_documents(documents, vertex_embeddings).as_retriever()\n"
      ],
      "metadata": {
        "id": "eTnaZxATGnOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#question = \"What potential issues are highlighted by the user 'Old_Lamplighter' when it comes to proofreading and formatting a CV for someone else?\"\n",
        "question = \"What potential issues are highlighted by the user 'Old_Lamplighter'?\"\n",
        "\n",
        "docs = vector_index.get_relevant_documents(question)\n",
        "context = \"\\n\".join(str(p.page_content) for p in docs)\n",
        "print(\"The total words in the context: \", len(context))\n",
        "print(docs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmXylXcpJsZA",
        "outputId": "01d25920-5a8f-46bd-9155-ddbcd3e3bb7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total words in the context:  771\n",
            "[Document(page_content=\"So, what can go bad is that the reviewer (or should we call him co-author?) might have his best intentions and embellish the candidate's profile, but the hiring manager would not be impressed.\", metadata={'seq_num': 39, 'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl'}), Document(page_content=\"So, what can go bad is that the reviewer (or should we call him co-author?) might have his best intentions and embellish the candidate's profile, but the hiring manager would not be impressed.\", metadata={'seq_num': 39, 'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl'}), Document(page_content=\"So, what can go bad is that the reviewer (or should we call him co-author?) might have his best intentions and embellish the candidate's profile, but the hiring manager would not be impressed.\", metadata={'seq_num': 39, 'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'start_index': 0}), Document(page_content=\"So, what can go bad is that the reviewer (or should we call him co-author?) might have his best intentions and embellish the candidate's profile, but the hiring manager would not be impressed.\", metadata={'seq_num': 39, 'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'start_index': 0})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_prompt_template = \"\"\"\n",
        "                    Answer the question as precise as possible using the provided context. \\n\\n\n",
        "                    Context: \\n {context} \\n\n",
        "                    Question: \\n {question} \\n\n",
        "                    Answer:\n",
        "                    \"\"\"\n",
        "question_prompt = PromptTemplate(\n",
        "    template=question_prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Combining Prompt\n",
        "combine_prompt_template = \"\"\"Given the extracted content and the question, create a final answer.\n",
        "If the answer is not contained in the context, say \"answer not available in context. \\n\\n\n",
        "Summaries: \\n {summaries}?\\n\n",
        "Question: \\n {question} \\n\n",
        "Answer:\n",
        "\"\"\"\n",
        "combine_prompt = PromptTemplate(\n",
        "    template=combine_prompt_template, input_variables=[\"summaries\", \"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "7aTnubCoL9ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_reduce_chain = load_qa_chain(\n",
        "    vertex_llm_text,\n",
        "    chain_type=\"map_reduce\",\n",
        "    return_intermediate_steps=True,\n",
        "    question_prompt=question_prompt,\n",
        "    combine_prompt=combine_prompt,\n",
        ")"
      ],
      "metadata": {
        "id": "krZSQWP_MBnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_reduce_embeddings_outputs = map_reduce_chain(\n",
        "    {\"input_documents\": docs, \"question\": question}\n",
        ")\n"
      ],
      "metadata": {
        "id": "axGZM4qFK2ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_reduce_embeddings_outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxVWWxeAMAV2",
        "outputId": "dc98018d-a83b-45d4-d8db-ea37380fe0b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_documents': [Document(page_content=\"So, what can go bad is that the reviewer (or should we call him co-author?) might have his best intentions and embellish the candidate's profile, but the hiring manager would not be impressed.\", metadata={'seq_num': 39, 'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl'}),\n",
              "  Document(page_content=\"So, what can go bad is that the reviewer (or should we call him co-author?) might have his best intentions and embellish the candidate's profile, but the hiring manager would not be impressed.\", metadata={'seq_num': 39, 'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl'}),\n",
              "  Document(page_content=\"So, what can go bad is that the reviewer (or should we call him co-author?) might have his best intentions and embellish the candidate's profile, but the hiring manager would not be impressed.\", metadata={'seq_num': 39, 'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'start_index': 0}),\n",
              "  Document(page_content=\"So, what can go bad is that the reviewer (or should we call him co-author?) might have his best intentions and embellish the candidate's profile, but the hiring manager would not be impressed.\", metadata={'seq_num': 39, 'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'start_index': 0})],\n",
              " 'question': \"What potential issues are highlighted by the user 'Old_Lamplighter'?\",\n",
              " 'intermediate_steps': [\"The reviewer might have his best intentions and embellish the candidate's profile, but the hiring manager would not be impressed.\",\n",
              "  \"The reviewer might have his best intentions and embellish the candidate's profile, but the hiring manager would not be impressed.\",\n",
              "  \"The reviewer might have his best intentions and embellish the candidate's profile, but the hiring manager would not be impressed.\",\n",
              "  \"The reviewer might have his best intentions and embellish the candidate's profile, but the hiring manager would not be impressed.\"],\n",
              " 'output_text': \"The reviewer might have his best intentions and embellish the candidate's profile, but the hiring manager would not be impressed.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "map_reduce_embeddings_outputs['output_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ctr_IRWSpCJ1",
        "outputId": "53ecbe3d-c447-4078-c153-a29692633796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The reviewer might have his best intentions and embellish the candidate's profile, but the hiring manager would not be impressed.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chroma with In Memory Disk"
      ],
      "metadata": {
        "id": "C8WaX2yJNLJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save to disk\n",
        "db2 = Chroma.from_documents(documents, vertex_embeddings, persist_directory=\"./chroma_db2\")"
      ],
      "metadata": {
        "id": "eHYfTwEBNNCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db2_retriever = db2.as_retriever(search_type=\"mmr\")\n",
        "question = \"What potential issues are highlighted by the user 'Old_Lamplighter' when it comes to proofreading and formatting a CV for someone else?\"\n",
        "retriever.get_relevant_documents(question)[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc0B5TE6i9jH",
        "outputId": "856a6516-0f57-41ef-e58e-9867eb67cafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Is it acceptable to format a cv for someone else?', metadata={'seq_num': 7, 'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl'})"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What potential issues are highlighted by the user 'Old_Lamplighter'?\"\n"
      ],
      "metadata": {
        "id": "mb_AVjTHqYkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db2_docs = db2.similarity_search_with_score(question)\n",
        "db2_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBS-hMNzjDXg",
        "outputId": "fa260dab-8b28-4883-c2b2-614444c62ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Document(page_content=\"So, what can go bad is that the reviewer (or should we call him co-author?) might have his best intentions and embellish the candidate's profile, but the hiring manager would not be impressed.\", metadata={'seq_num': 39, 'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'start_index': 0}),\n",
              "  0.7055971026420593),\n",
              " (Document(page_content='There are plenty of people whose examples provide a questionable thought process - \"I wanted to use it because it was new even though it might not have been a good fit/', metadata={'seq_num': 154, 'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'start_index': 0}),\n",
              "  0.7229040861129761),\n",
              " (Document(page_content='Check out their Stack Overflow profile - are they actively asking/answering questions?', metadata={'seq_num': 263, 'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'start_index': 0}),\n",
              "  0.7255362868309021),\n",
              " (Document(page_content='It can possibly be a problem, IF.', metadata={'seq_num': 26, 'source': '/content/drive/MyDrive/0.PhD/Jarvis-QA/validation.jsonl', 'start_index': 0}),\n",
              "  0.728373646736145)]"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db3 = Chroma(persist_directory=\"./chroma_db\", embedding_function=vertex_embeddings)\n",
        "ret_docs = db3.similarity_search(question)\n",
        "context = \"\\n\".join(str(p.page_content) for p in ret_docs)\n",
        "print(\"The total words in the context: \", len(context))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ_kA6iRNRfn",
        "outputId": "e9506edd-a568-4020-ecf0-e20b728208e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total words in the context:  723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Answer the question as precise as possible using the provided context. If the answer is\n",
        "                    not contained in the context, say \"answer not available in context\" \\n\\n\n",
        "                    Context: \\n {context}?\\n\n",
        "                    Question: \\n {question} \\n\n",
        "                    Answer:\n",
        "                  \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "vector_store_chain = load_qa_chain(vertex_llm_text, chain_type=\"stuff\", prompt=prompt)\n",
        "vector_store_answer = stuff_chain(\n",
        "    {\"input_documents\": ret_docs, \"question\": question}, return_only_outputs=True\n",
        ")\n",
        "pprint(vector_store_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_vXjBJRNhg0",
        "outputId": "b115822e-b4d1-4c01-e7e3-0210239d99c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'output_text': ' The reviewer might have his best intentions and embellish '\n",
            "                \"the candidate's profile, but the hiring manager would not be \"\n",
            "                'impressed.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. State of the Art - LangChain Expression Language (LCEL)"
      ],
      "metadata": {
        "id": "wjBrOkGFPu0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": vector_index | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | vertex_llm_text\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "LNI6nM6gPe48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What potential issues are highlighted by the user 'Old_Lamplighter'?\"\n",
        "for chunk in rag_chain.stream(question):\n",
        "    print(chunk, end=\"\", flush=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_I39qpVRfWx",
        "outputId": "860a1f6b-3bc6-466e-e9c9-29061d5146e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the reviewer (or should we call him co-author?) might have his best intentions and embellish the candidate's profile, but the hiring manager would not be impressed."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Exploration of Smallest LLMs"
      ],
      "metadata": {
        "id": "2MR5rz7eT5n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install huggingface_hub\n",
        "\n"
      ],
      "metadata": {
        "id": "vja33xbVRp3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2GEkP_YT9Vd",
        "outputId": "d6e8bfc7-6b13-4e69-fcde-c9fbf307b99e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
      ],
      "metadata": {
        "id": "QSzV7XDdUAZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "WwM7bgaRU68C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Flan-T5 :"
      ],
      "metadata": {
        "id": "cdjpN0RkWWgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"google/flan-t5-xxl\"\n",
        "flan_llm = HuggingFaceHub(\n",
        "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 1000}\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFKp9FDuVk1J",
        "outputId": "058f62fb-31a0-47f0-b728-ab050594e10b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flan_rag_chain = (\n",
        "    {\"context\": vector_index | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | flan_llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "QvStZpYyVIYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What potential issues are highlighted by the user 'Old_Lamplighter'?\"\n",
        "\n",
        "flan_rag_chain.invoke(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DTrlVupAVtq3",
        "outputId": "71e6c58d-2893-4dd4-dfca-818045cf8ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'answer not available in context'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Mistral 7B"
      ],
      "metadata": {
        "id": "zknlhel3Z7Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "mistral_llm = HuggingFaceHub(\n",
        "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 1000}\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dN06Q-iV8-m",
        "outputId": "22a0368a-484a-45ef-cc7c-e5234bf93e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mistral_rag_chain = (\n",
        "    {\"context\": vector_index | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | mistral_llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "BH1MRhWEaVRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What potential issues are highlighted by the user 'Old_Lamplighter'?\"\n",
        "\n",
        "mistral_rag_chain.invoke(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZUfcetJkaYvr",
        "outputId": "e7abbc64-9b4b-42c2-8740-88f41e0ac305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" The potential issues highlighted by the user 'Old_Lamplighter' are that the reviewer\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. Zephyr 7B"
      ],
      "metadata": {
        "id": "r11DmdyFaeAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "zephyr_llm = HuggingFaceHub(\n",
        "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 1000}\n",
        ")\n"
      ],
      "metadata": {
        "id": "ox4rc8zoaZy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zephyr_rag_chain = (\n",
        "    {\"context\": vector_index | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | zephyr_llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "lklyBsrpbZFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What potential issues are highlighted by the user 'Old_Lamplighter'?\"\n",
        "zephyr_rag_chain.invoke(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "2caNCd98bfh8",
        "outputId": "e9daf08e-42ad-4b9c-b885-5c2d3cb127b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" The user 'Old_Lamplighter' highlights the potential issue of a reviewer (or co-author) embellishing a candidate's profile, which may not impress the hiring manager. This could result in the candidate's qualifications being overstated or exaggerated, potentially leading to false expectations or disappointment during the hiring process. The user's concern is that the hiring manager may not be impressed by the embellished profile, which could negatively impact the candidate's chances of being hired.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4. Other Models\n",
        "`1. Alpaca 7B`\n",
        "\n",
        "`2. Llama 7B / Open Llama 3B`\n",
        "\n",
        "`3. BLING Series of Models`"
      ],
      "metadata": {
        "id": "96I1xHz-b4V1"
      }
    }
  ]
}
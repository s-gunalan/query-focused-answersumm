{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import warnings\n",
    "from pathlib import Path as p\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_llm_text = VertexAI(model_name=\"text-bison@001\")\n",
    "vertex_embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@001\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'train.jsonl'\n",
    "from pathlib import Path\n",
    "\n",
    "pprint(Path(dataset_path).read_text())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextGenerationModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import JSONLoader\n",
    "dataset_path = 'validation.jsonl'\n",
    "loader = JSONLoader(\n",
    "    file_path=dataset_path,\n",
    "    jq_schema='.answers[].sents[].text',\n",
    "    text_content=False,\n",
    "    json_lines=True)\n",
    "\n",
    "data = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'alkali-gworks'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Stuffing (Single Shot Prompting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\".join(str(p.page_content) for p in data[:7])\n",
    "print(\"The total words in the context: \", len(context))\n",
    "\n",
    "question = \"Is it acceptable to format a cv for someone elsen?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Answer the question as precise as possible using the provided context. If the answer is\n",
    "                    not contained in the context, say \"answer not available in context\" \\n\\n\n",
    "                    Context: \\n {context}?\\n\n",
    "                    Question: \\n {question} \\n\n",
    "                    Answer:\n",
    "                  \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "stuff_chain = load_qa_chain(vertex_llm_text, chain_type=\"stuff\", prompt=prompt)\n",
    "stuff_answer = stuff_chain(\n",
    "    {\"input_documents\": data[7:10], \"question\": question}, return_only_outputs=True\n",
    ")\n",
    "pprint(stuff_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MapReduce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\".join(str(p.page_content) for p in data[:7])\n",
    "print(\"The total words in the context: \", len(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompt_template = \"\"\"\n",
    "                    Answer the question as precise as possible using the provided context. \\n\\n\n",
    "                    Context: \\n {context} \\n\n",
    "                    Question: \\n {question} \\n\n",
    "                    Answer:\n",
    "                    \"\"\"\n",
    "question_prompt = PromptTemplate(\n",
    "    template=question_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# summaries is required. a bit confusing.\n",
    "combine_prompt_template = \"\"\"Given the extracted content and the question, create a final answer.\n",
    "If the answer is not contained in the context, say \"answer not available in context. \\n\\n\n",
    "Summaries: \\n {summaries}?\\n\n",
    "Question: \\n {question} \\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template, input_variables=[\"summaries\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_reduce_chain = load_qa_chain(\n",
    "    vertex_llm_text,\n",
    "    chain_type=\"map_reduce\",\n",
    "    return_intermediate_steps=True,\n",
    "    question_prompt=question_prompt,\n",
    "    combine_prompt=combine_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is it acceptable to format a cv for someone elsen?\"\n",
    "map_reduce_outputs = map_reduce_chain({\"input_documents\": data[:100], \"question\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_reduce_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_prompt_template = \"\"\"\n",
    "    The original question is: \\n {question} \\n\n",
    "    The provided answer is: \\n {existing_answer}\\n\n",
    "    Refine the existing answer if needed with the following context: \\n {context_str} \\n\n",
    "    Given the extracted content and the question, create a final answer.\n",
    "    If the answer is not contained in the context, say \"answer not available in context. \\n\\n\n",
    "\"\"\"\n",
    "refine_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"existing_answer\", \"context_str\"],\n",
    "    template=refine_prompt_template,\n",
    ")\n",
    "\n",
    "\n",
    "initial_question_prompt_template = \"\"\"\n",
    "    Answer the question as precise as possible using the provided context only. \\n\\n\n",
    "    Context: \\n {context_str} \\n\n",
    "    Question: \\n {question} \\n\n",
    "    Answer:\n",
    "\"\"\"\n",
    "\n",
    "initial_question_prompt = PromptTemplate(\n",
    "    input_variables=[\"context_str\", \"question\"],\n",
    "    template=initial_question_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_chain = load_qa_chain(\n",
    "    vertex_llm_text,\n",
    "    chain_type=\"refine\",\n",
    "    return_intermediate_steps=True,\n",
    "    question_prompt=initial_question_prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is it acceptable to format a cv for someone elsen?\"\n",
    "\n",
    "refine_outputs = refine_chain({\"input_documents\": data[:20], \"question\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Q&A with similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=10000, chunk_overlap=0)\n",
    "context_ss = \"\\n\\n\".join(str(p.page_content) for p in data)\n",
    "texts_ss = text_splitter.split_text(context_ss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = Chroma.from_texts(texts_ss, vertex_embeddings).as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vector_index.get_relevant_documents(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is it acceptable to format a cv for someone elsen?\"\n",
    "map_reduce_embeddings_outputs = map_reduce_chain(\n",
    "    {\"input_documents\": docs, \"question\": question}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(map_reduce_embeddings_outputs[\"output_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextGenerationModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "vertexai.init(project=\"alkali-gworks\", location=\"us-central1\")\n",
    "parameters = {\n",
    "    \"candidate_count\": 1,\n",
    "    \"max_output_tokens\": 1024,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 40\n",
    "}\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison\")\n",
    "response = model.predict(\n",
    "    \"\"\"Hi\"\"\",\n",
    "    **parameters\n",
    ")\n",
    "print(f\"Response from Model: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "aiplatform.init(project=\"alkali-gworks\", location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "\n",
    "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
    "import langchain\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
